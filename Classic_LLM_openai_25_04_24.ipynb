{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andipasti/smupo/blob/main/Classic_LLM_openai_25_04_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNSHHKfaCvHu"
      },
      "source": [
        "# MASTER THESIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DxzokNC9D5"
      },
      "source": [
        "### INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAHuRdyoMU_e"
      },
      "outputs": [],
      "source": [
        "#SETTING GLOBALS\n",
        "\n",
        "DF_SMALL = './drive/Othercomputers/X1/code/datas_small.json'\n",
        "DF_MID = './drive/Othercomputers/X1/code/datas_mid.json'\n",
        "DF_LARGE = './drive/Othercomputers/X1/code/datas_large.json'\n",
        "\n",
        "DF_SMALL_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_small.json'\n",
        "DF_MID_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_mid.json'\n",
        "DF_LARGE_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_large.json'\n",
        "\n",
        "EMBEDDINGS_SMALL = './drive/Othercomputers/X1/code/embeddings/embeddings-small-initial.npy'\n",
        "EMBEDDINGS_MID = './drive/Othercomputers/X1/code/embeddings/embeddings-mid-initial.npy'\n",
        "EMBEDDINGS_LARGE = './drive/Othercomputers/X1/code/embeddings/embeddings-large-initial.npy'\n",
        "\n",
        "EMBEDDINGS_SMALL_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "EMBEDDINGS_MID_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "EMBEDDINGS_LARGE_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "\n",
        "MAX_DISTANCE = 2;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jgv2mj-CVxL",
        "outputId": "c9832f68-2441-4426-898b-4edef4f1af52",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m986.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.0\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.12-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.4.0)\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.12 umap-learn-0.5.6\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# INSTALL NEEDED LIBRARIES\n",
        "!pip install openai==0.28\n",
        "!pip install umap-learn\n",
        "!pip install wordcloud\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizers\n",
        "nltk.download('stopwords')  # Stopwords list for English and German\n",
        "\n",
        "# Set up stopwords for both German and English\n",
        "stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "import umap.umap_ as umap\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2rVdhzOmTYlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dW94RB9B4Iu"
      },
      "outputs": [],
      "source": [
        "# CREATE ACCESS TO SECRETS\n",
        "# Now you can proceed with your code that requires this environment variable\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACE_TOKEN'] = userdata.get('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2usZnsPDTPX",
        "outputId": "aad1eba7-7df5-4dab-e594-e2eec638b696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7fGWY63DT_4",
        "outputId": "76b2b638-ecc2-4a69-8ef7-48744e1c527a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key is set.\n"
          ]
        }
      ],
      "source": [
        "# CHECKING\n",
        "\n",
        "# Check if the OPENAI_API_KEY environment variable is set\n",
        "if \"OPENAI_API_KEY\" in os.environ:\n",
        "    print(\"OpenAI API key is set.\")\n",
        "else:\n",
        "    print(\"OpenAI API key is not set.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x5_GAVb7BCT"
      },
      "outputs": [],
      "source": [
        "# CONNECT TO DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmY0vi2TDaUJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBwl74V5DFtC"
      },
      "source": [
        "### GETTING DATAS AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSfkp7fiDYFi"
      },
      "outputs": [],
      "source": [
        "# LOAD DATAS\n",
        "def load_datas(config_key):\n",
        "  print(config_key)\n",
        "  df = pd.read_json(config_key)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0bVk2l_Ddun"
      },
      "outputs": [],
      "source": [
        "def print_column_names(df):\n",
        "  column_names = list(df.columns)\n",
        "  print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPTyUg0_Dfii"
      },
      "outputs": [],
      "source": [
        "def drop_columns(df, columns):\n",
        "  return df.drop(columns=columns, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNCqVvbEpbZj"
      },
      "outputs": [],
      "source": [
        "def replace_special_signs(df, column):\n",
        "  df[column] = df[column].str.replace(r\"[\\n\\r]\", \" \", regex=True)\n",
        "  df[column] = df[column].str.replace(r\"[^a-zA-Z0-9\\s.,;!?äüöéèà']\", \"\", regex=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "MyYaQBSBmeWK"
      },
      "outputs": [],
      "source": [
        "def combine_columns(df, source_cols, new_col_name, separator=\" \"):\n",
        "  df[new_col_name] = df[source_cols].astype(str).agg(separator.join, axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "najjKVvEyQsq"
      },
      "source": [
        "### RESTRUCTURE TICKET INFORMATION WITH HELP OF LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kv3AfNCjy7Ux"
      },
      "outputs": [],
      "source": [
        "def rework_ticket_description_with_gptturbo(ticket_text, detailed_instructions, model):\n",
        "    \"\"\"\n",
        "    Reworks a ticket description into a structured summary using the specified GPT model.\n",
        "\n",
        "    :param ticket_text: The text of the ticket.\n",
        "    :param detailed_instructions: Detailed instructions for the model.\n",
        "    :param model: The model identifier to use.\n",
        "    :return: Reworked ticket text.\n",
        "    \"\"\"\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": detailed_instructions},\n",
        "                {\"role\": \"user\", \"content\": ticket_text}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if response and 'choices' in response and len(response['choices']) > 0:\n",
        "            reworked_text = response['choices'][0].get('message', {}).get('content', '')\n",
        "            return reworked_text.strip()\n",
        "        else:\n",
        "            return \"Failed to generate reworked text. No valid response.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CC9TX39JDs6w"
      },
      "outputs": [],
      "source": [
        "def rework_ticket_information(file_path, df, detailed_instructions, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Check if a processed file exists. If not, process the tickets using GPT-Turbo and save the new data.\n",
        "\n",
        "    :param file_path: Path to the processed file.\n",
        "    :param df: DataFrame containing the ticket descriptions.\n",
        "    :param detailed_instructions: Instructions for GPT model summarization.\n",
        "    :param model: The model identifier to use with OpenAI's API.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        # If the file does not exist, apply the function to rework ticket descriptions\n",
        "        df['reworked_information'] = df['combined_text'].apply(lambda x: rework_ticket_description_with_gptturbo(x, detailed_instructions, model))\n",
        "        df.to_json(file_path, orient='records', lines=True)\n",
        "        files.download(file_path)\n",
        "        print(\"Processed new data and saved to file.\")\n",
        "    else:\n",
        "        # Load the existing file if it already exists\n",
        "        df = pd.read_json(file_path, lines=True)\n",
        "        print(\"File exists. Loaded existing data.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHzdJsnIFyYM",
        "outputId": "46e2cffc-e29c-478c-cde6-4d3d942c3fd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "def clean_datas_in_col(text):\n",
        "    \"\"\" Cleans the text data by removing specific words and non-alphabetical characters. \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if text is not a string\n",
        "\n",
        "    text = text.lower()\n",
        "    specific_unwanted_words = ['reported problem', 'ticket', 'summary', 'affected systems', 'reported problem', 'issue']\n",
        "    pattern = r'\\b(?:' + '|'.join(specific_unwanted_words) + r')\\b'\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def clean_datas(df, column_name):\n",
        "    \"\"\" Applies text cleaning to a specified column in the DataFrame. \"\"\"\n",
        "    df[column_name + '_cleaned'] = df[column_name].apply(clean_datas_in_col)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYs16m3EhmV"
      },
      "source": [
        "## CREATE EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "-qivnbJ8Vdaj"
      },
      "outputs": [],
      "source": [
        "# USE OPENAI MODEL TO CREATE EMBEDDINGS OUT OF REWORKED INFORMATION\n",
        "\n",
        "# Function to get embeddings from OpenAI\n",
        "def _impl(texts, embeddings_file_path):\n",
        "\n",
        "  # Explicitly set the OpenAI API key\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "  if not os.path.exists(embeddings_file_path):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        response = openai.Embedding.create(\n",
        "            input=text,\n",
        "            engine=\"text-embedding-ada-002\"\n",
        "        )\n",
        "        embeddings.append(response['data'][0]['embedding'])\n",
        "\n",
        "    # save:\n",
        "    np.save(embeddings_file_path, embeddings)\n",
        "  else:\n",
        "    embeddings = np.load(embeddings_file_path)\n",
        "    print(\"EMBEDDING read: \", embeddings.shape[0])\n",
        "\n",
        "  return np.array(embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30CUT37DImGB"
      },
      "source": [
        "TODO:  \n",
        "- different models\n",
        "- optimize preprocessing (normalize, stopwords, lemmatization,...)\n",
        "- fine-tuning embeddings /hybrid approach (using z.b randomForest after clustering)\n",
        "- dimensionality reduction (PCA f.e.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ckKNThE5tT"
      },
      "source": [
        "## CREATE CLUSTERS V1\n",
        "\n",
        "### WARD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ACXwP7m5yI"
      },
      "outputs": [],
      "source": [
        "# Perform hierarchical clustering\n",
        "\n",
        "Z = linkage(matrix, 'ward')\n",
        "\n",
        "# Plot the dendrogram to visualize clusters\n",
        "plt.figure(figsize=(25, 10))\n",
        "dendrogram(\n",
        "    Z,\n",
        "    leaf_rotation=90.,  # rotates the x axis labels\n",
        "    leaf_font_size=8.,  # font size for the x axis labels\n",
        ")\n",
        "plt.title('Hierarchical Clustering Dendrogram')\n",
        "plt.xlabel('Sample index or (Cluster size)')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "2eJVGbEUQZhJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "ijRAk70d-Azf"
      },
      "outputs": [],
      "source": [
        "def perform_dbscan_clustering(matrix):\n",
        "  print(\"IM PERFORM DBSCAN\")\n",
        "  dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
        "  clusters = dbscan.fit_predict(matrix)\n",
        "  return clusters\n",
        "\n",
        "# Optional: Plot clusters if you wish to visualize\n",
        "# plot_clusters_with_umap(matrix, clusters)\n",
        "\n",
        "\n",
        "def plot_clusters(matrix, clusters):\n",
        "    # Perform dimensionality reduction for visualization if it's not 2D\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    matrix_2d = tsne.fit_transform(matrix)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(matrix_2d[:, 0], matrix_2d[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('DBSCAN Clustering Visualization')\n",
        "    plt.xlabel('TSNE Dimension 1')\n",
        "    plt.ylabel('TSNE Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import umap.umap_ as umap\n",
        "\n",
        "def plot_clusters_with_umap(matrix, clusters):\n",
        "    # Use UMAP for dimensionality reduction\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    matrix_2d = reducer.fit_transform(matrix)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(matrix_2d[:, 0], matrix_2d[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('DBSCAN Clustering Visualization with UMAP')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbE_G5lzbKme"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Assuming 'matrix' is your embeddings matrix obtained from get_openai_embeddings function\n",
        "def perform_kmeans_clustering(matrix, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
        "    clusters = kmeans.fit_predict(matrix)\n",
        "    return clusters\n",
        "\n",
        "# Example usage\n",
        "num_clusters = 3  # Based on your elbow method result\n",
        "kmeans_clusters = perform_kmeans_clustering(matrix, num_clusters)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "def perform_spectral_clustering(matrix, n_clusters):\n",
        "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',\n",
        "                                  assign_labels='kmeans', random_state=42)\n",
        "    clusters = spectral.fit_predict(matrix)\n",
        "    return clusters\n",
        "\n",
        "n_clusters = 3\n",
        "spectral_clusters = perform_spectral_clustering(matrix, n_clusters)\n"
      ],
      "metadata": {
        "id": "CTslVfmWTvMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF70HCiixtAY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrJIlT6BFXeC"
      },
      "source": [
        "### CHECK HOW MANY CLUSTERS WOULD BE USEFUL (KMEANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd3gKlbhm7II"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# Check with kmeans to be sure about amount of clusters\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'matrix' is the embeddings matrix obtained from get_openai_embeddings function\n",
        "wcss = []\n",
        "for i in range(1, 11):  # Test for k values from 1 to 10, adjust the range as necessary\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=15, random_state=0)\n",
        "    kmeans.fit(matrix)\n",
        "    wcss.append(kmeans.inertia_)  # Inertia: Sum of distances of samples to their closest cluster center\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, 31), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')  # Within-cluster sum of squares\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8AyVIj2JIGG"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFciMxR4agAu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "import umap\n",
        "\n",
        "def perform_hierarchical_clustering(embeddings, MAX_DISTANCE):\n",
        "    Z = linkage(embeddings, 'ward')\n",
        "    clusters = fcluster(Z, MAX_DISTANCE, criterion='distance')\n",
        "    print(\"Number of clusters formed:\", len(np.unique(clusters)))\n",
        "    return clusters, Z\n",
        "\n",
        "def visualize_umap(embeddings, clusters):\n",
        "    umap_reducer = umap.UMAP()\n",
        "    umap_result = umap_reducer.fit_transform(embeddings)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('UMAP Visualization of Clusters')\n",
        "    plt.xlabel('UMAP Feature 1')\n",
        "    plt.ylabel('UMAP Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIV_ksNQF8TA"
      },
      "outputs": [],
      "source": [
        "# EXCECUTE IT\n",
        "# Perform clustering\n",
        "clusters, Z = perform_hierarchical_clustering(embeddings, MAX_DISTANCE)\n",
        "df['cluster_label'] = clusters  # Assign cluster labels to DataFrame\n",
        "\n",
        "# Visualize using UMAP\n",
        "visualize_umap(embeddings, clusters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_jG6k2rKkev"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Download stopwords and wordnet for NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set up stopwords for both German and English\n",
        "stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooqwllfp5iH9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')  # Tokenizers\n",
        "nltk.download('stopwords')  # Stopwords list for English and German\n",
        "\n",
        "# Set up stopwords for both German and English\n",
        "stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Check if the text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if text is not a string\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove specific unwanted words\n",
        "    specific_unwanted_words = ['reported problem', 'support', 'ticket', 'summary', 'affected systems', 'reported problem', 'user', 'issue']\n",
        "    pattern = r'\\b(?:' + '|'.join(specific_unwanted_words) + r')\\b'\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    # Tokeniation and stopwords removal\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Assuming df is your DataFrame and 'reworked_information' is the column with texts\n",
        "df['cleaned_information'] = df['reworked_information'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjltFvRU6Ql4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVj9le13Uot-"
      },
      "source": [
        "CHECKPOINT 1: EVALUATE CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "def compute_clustering_metrics(matrix, labels):\n",
        "    if len(np.unique(labels)) > 1:  # More than one cluster is required\n",
        "        silhouette_avg = silhouette_score(matrix, labels)\n",
        "        davies_bouldin = davies_bouldin_score(matrix, labels)\n",
        "        return silhouette_avg, davies_bouldin\n",
        "    else:\n",
        "        return None, None  # Not enough clusters to compute metrics\n"
      ],
      "metadata": {
        "id": "fLORVenjQu9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_clusters_with_umap(matrix, clusters, title):\n",
        "    umap_reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = umap_reducer.fit_transform(matrix)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('UMAP Feature 1')\n",
        "    plt.ylabel('UMAP Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "# Usage example\n",
        "plot_clusters_with_umap(matrix, kmeans_clusters, 'K-Means Clustering with UMAP')\n",
        "plot_clusters_with_umap(matrix, clusters, 'DBSCAN Clustering with UMAP')  # Assuming 'clusters' from DBSCAN\n",
        "#plot_clusters_with_umap(matrix, cluster_labels, 'Hierarchical Clustering with UMAP')  # Assuming 'cluster_labels' from Ward\n",
        "\n",
        "\n",
        "plot_clusters_with_umap(matrix, spectral_clusters, 'Spectral Clustering with UMAP')\n"
      ],
      "metadata": {
        "id": "oRsUnH70QyHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate DBSCAN\n",
        "db_silhouette, db_davies = compute_clustering_metrics(matrix, clusters)\n",
        "# Evaluate Hierarchical\n",
        "#ward_silhouette, ward_davies = compute_clustering_metrics(matrix, cluster_labels)\n",
        "# Evaluate K-Means\n",
        "kmeans_silhouette, kmeans_davies = compute_clustering_metrics(matrix, kmeans_clusters)\n",
        "\n",
        "# Print the metrics for comparison\n",
        "print(f\"DBSCAN Silhouette: {db_silhouette}, DBSCAN Davies-Bouldin: {db_davies}\")\n",
        "#print(f\"Ward Silhouette: {ward_silhouette}, Ward Davies-Bouldin: {ward_davies}\")\n",
        "print(f\"K-Means Silhouette: {kmeans_silhouette}, K-Means Davies-Bouldin: {kmeans_davies}\")\n",
        "\n",
        "spectral_silhouette, spectral_davies = compute_clustering_metrics(matrix, spectral_clusters)\n",
        "print(f\"Spectral Clustering Silhouette Score: {spectral_silhouette}\")\n",
        "print(f\"Spectral Clustering Davies-Bouldin Index: {spectral_davies}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SRHWnr4cQ0Dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqQjUjPAUumK"
      },
      "outputs": [],
      "source": [
        "#Silhouette Score WARD\n",
        "# FCLUSTER\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Assuming the rest of your code has been executed up to the clustering\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "# Assuming 'Z' is your linkage matrix from hierarchical clustering\n",
        "# and 'MAX_DISTANCE' is your threshold for clustering\n",
        "cluster_labels = fcluster(Z, MAX_DISTANCE, criterion='distance')\n",
        "df['cluster_label'] = cluster_labels\n",
        "\n",
        "# Now calculate the silhouette score\n",
        "# Ensure your embeddings are appropriately prepared as 'matrix'\n",
        "silhouette_avg = silhouette_score(matrix, cluster_labels)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Continue with any further steps you have\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yff6ERg0b01k"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming matrix contains your embeddings\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust these parameters based on your data\n",
        "clusters = dbscan.fit_predict(matrix)\n",
        "\n",
        "# Filter out noise points for silhouette calculation (-1 labels represent noise in DBSCAN)\n",
        "filtered_labels = clusters[clusters != -1]\n",
        "filtered_matrix = matrix[clusters != -1]\n",
        "\n",
        "# Count the unique cluster labels (excluding noise)\n",
        "num_clusters = len(np.unique(filtered_labels))\n",
        "\n",
        "if num_clusters > 1:  # Ensure there's more than one cluster to calculate the score\n",
        "    silhouette_avg = silhouette_score(filtered_matrix, filtered_labels)\n",
        "    print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "    print(f\"Number of clusters (excluding noise): {num_clusters}\")\n",
        "else:\n",
        "    print(\"Not enough clusters to calculate a meaningful silhouette score.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCmic5nZXI9d"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFgdBQM-U0x0"
      },
      "outputs": [],
      "source": [
        "# Davies-Bouldin Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej180bnULQF3"
      },
      "source": [
        "SHOWING IN A WORD CLOUD - CLUSTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUmX6uWZLPaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNmOjtIDu05P"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS5N9k8xLXa6"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def generate_wordclouds(df):\n",
        "    unique_clusters = np.unique(df['cluster_label'])\n",
        "    print(\"UNIQUE CLUSTERS: \", unique_clusters)\n",
        "    for cluster_num in unique_clusters:\n",
        "\n",
        "        cluster_data = df[df['cluster_label'] == cluster_num]\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_data['cleaned_information'])\n",
        "        terms = tfidf_vectorizer.get_feature_names_out()\n",
        "        scores = tfidf_matrix.sum(axis=0).A1\n",
        "        freqs = dict(zip(terms, scores))\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freqs)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f'Word Cloud for Cluster {cluster_num}')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CO44BupiShP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def generate_cluster_labels(df):\n",
        "    unique_clusters = np.unique(df['cluster_label'])\n",
        "    labels = []\n",
        "    print(\"CLUSTERS IN GENERATE CLUSTER FUNC \", unique_clusters)\n",
        "    for cluster_num in unique_clusters:\n",
        "\n",
        "        cluster_data = df[df['cluster_label'] == cluster_num]\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_data['cleaned_information'])\n",
        "        terms = np.array(tfidf_vectorizer.get_feature_names_out())[tfidf_matrix.sum(axis=0).argsort()[0, ::-1]].flat[:5]\n",
        "        label = ', '.join(terms)\n",
        "        labels.append(label)\n",
        "        print(f\"Cluster {cluster_num} label: {label}\")\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY840vaXHZ-m"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Generate meaningful labels for each cluster using GPT-3.5 Turbo\n",
        "def create_gpt3_labels(labels):\n",
        "    cluster_labels = []\n",
        "    for label in labels:\n",
        "\n",
        "        #prompt = \"Please generate a concise label for the following cluster of terms.\"\n",
        "        prompt = f\"Generate a very specific and descriptive maximum 3-words-label for a cluster of topics involving: {label}. Aim for precision and relevance in a technical context. it is all about a webapplication where users like teachers or scholars can see timetable, do enrolments, add and confirm absences and many more.\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": label}\n",
        "            ]\n",
        "        )\n",
        "        new_label = response['choices'][0]['message']['content'].strip()\n",
        "        cluster_labels.append(new_label)\n",
        "    return cluster_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1mbKxmWHxDA"
      },
      "outputs": [],
      "source": [
        "# Perform clustering\n",
        "\n",
        "clusters, Z = perform_hierarchical_clustering(embeddings, MAX_DISTANCE)\n",
        "\n",
        "df['cluster_label'] = clusters  # Assign cluster labels to DataFrame\n",
        "\n",
        "# Visualize using UMAP\n",
        "\n",
        "# Generate word clouds for each cluster\n",
        "generate_wordclouds(df)\n",
        "\n",
        "\n",
        "# Generate labels using TF-IDF\n",
        "labels = generate_cluster_labels(df)\n",
        "\n",
        "\n",
        "# Create meaningful labels using GPT-3.5 Turbo\n",
        "cluster_labels_reworked = create_gpt3_labels(labels)\n",
        "\n",
        "\n",
        "# Create and display final cluster information\n",
        "cluster_data = pd.DataFrame({\n",
        "    'ClusterNumber': range(1, len(cluster_labels_reworked) + 1),\n",
        "    'ClusterLabel': cluster_labels_reworked,\n",
        "    'OriginalLabels': labels\n",
        "})\n",
        "\n",
        "print(cluster_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJdAcuyStmda"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rfMxWQR8ehQ"
      },
      "source": [
        "CATEGORIZING NEW TICKETS TO ONE OF THE CLUSTERS WITHOUT THE NEED OF DOING THE WHOLE CLUSTERSTUFF ETC AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6mC3W6kLOJU"
      },
      "outputs": [],
      "source": [
        "def plot_umap(embeddings, cluster_labels, new_ticket_index):\n",
        "    reducer = umap.UMAP()\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "    plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', alpha=0.6)\n",
        "    plt.scatter(embedding_2d[new_ticket_index, 0], embedding_2d[new_ticket_index, 1], c='red', s=100, edgecolors='k')  # Highlight new ticket\n",
        "    plt.title(\"UMAP Projection of Ticket Embeddings\")\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4w5-xAkrxxc"
      },
      "outputs": [],
      "source": [
        "def check_new_ticket(embeddings, reworked_ticket_text, df, update_path, threshold=0.5):\n",
        "    print(\"Starting check_new_ticket...\")\n",
        "\n",
        "    # get last embedding\n",
        "    new_embedding = embeddings[-1]\n",
        "    all_embeddings_exept_last = embeddings[:-1]\n",
        "\n",
        "    # Check if the DataFrame is empty or the existing_embeddings are insufficient\n",
        "    if df.empty or len(embeddings) <= 1:  # Ensure there's at least one existing ticket besides the new one\n",
        "        print(\"No existing tickets to compare or DataFrame is empty.\")\n",
        "        new_label = 0  # Starting with a new cluster if no valid data exists\n",
        "    else:\n",
        "        # Exclude the new embedding which is presumably the last one added\n",
        "        print(new_embedding)\n",
        "        # Calculate distances to all existing embeddings\n",
        "        distances = euclidean_distances([new_embedding], all_embeddings_exept_last)\n",
        "        min_distance = distances.min()\n",
        "\n",
        "        print(\"Distance: \", distances.min())\n",
        "        # Decide on the cluster label based on distance\n",
        "        if min_distance > threshold:\n",
        "          print(\"NEUES CLUSTER\")\n",
        "          new_label = df['cluster_label'].max() + 1 if not df.empty else 0\n",
        "          #new_label = 0\n",
        "          print(f\"Assigning new cluster label: {new_label}\")\n",
        "        else:\n",
        "            print(\"BESTEHENDES CLUSTER\")\n",
        "            closest_index = distances.argmin()\n",
        "            print(\"CLOSEST INDEX: \", closest_index)\n",
        "            if closest_index < len(df):\n",
        "                new_label = df.iloc[closest_index]['cluster_label']\n",
        "                #new_label = 0\n",
        "                print(f\"Assigning existing cluster label: {new_label}\")\n",
        "            else:\n",
        "                print(f\"Error: Closest index {closest_index} out of bounds for DataFrame of length {len(df)}\")\n",
        "                return df, None  # Return None if there's an index error\n",
        "\n",
        "    # Append the new ticket's data as a new row to the DataFrame\n",
        "\n",
        "    df.loc[len(df)-1, 'cluster_label'] =  new_label\n",
        "\n",
        "    new_ticket_index = df.index[-1]  # Get the index of the newly added ticket\n",
        "    return df, new_ticket_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER20SqhMr0aB"
      },
      "outputs": [],
      "source": [
        "def get_openai_embeddings_addon(texts, initial_path, update_path):\n",
        "    # Check and load initial embeddings or initialize an empty array\n",
        "\n",
        "    new_embeddings = []\n",
        "    for text in texts:\n",
        "        response = openai.Embedding.create(input=text, engine=\"text-embedding-3-large\"\")\n",
        "        new_embeddings.append(response['data'][0]['embedding'])\n",
        "\n",
        "    return new_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hijF2duoztdt"
      },
      "source": [
        "## CREATE ANOTHER TICKET AND RECLUSTER IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw40kkhx81WU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "initial_path = EMBEDDINGS_MID\n",
        "update_path = EMBEDDINGS_MID_UPDATED\n",
        "new_ticket = \"eine gurke kommt plötzlich aus meinem handy anstelle einer banane.\"\n",
        "\n",
        "reworked_ticket_text = rework_ticket_description_with_gptturbo(new_ticket)\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")\n",
        "df.loc[len(df), ['issue_title', 'reworked_information', 'cleaned_information']] = [\"MY TEST TITLE FOR NEW ISSUES\", reworked_ticket_text, reworked_ticket_text]\n",
        "\n",
        "\n",
        "new_embeddings = get_openai_embeddings_addon([reworked_ticket_text], initial_path, update_path)[-1]\n",
        "\n",
        "# Combine new embeddings with existing ones\n",
        "new_embeddings = np.array(new_embeddings)\n",
        "embeddings = np.vstack([embeddings, new_embeddings]) if embeddings.size else new_embeddings\n",
        "\n",
        "# Save updated embeddings\n",
        "np.save(update_path, embeddings)\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")\n",
        "\n",
        "#embeddings = np.load(update_path)[:-2]\n",
        "#embeddings = np.load(update_path)[:-1]\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzVXwUoLaVEu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XDFzwwIGVtX"
      },
      "outputs": [],
      "source": [
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxjfkEc7P7SC"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3G07cGGZ_tE"
      },
      "outputs": [],
      "source": [
        "#df = df.drop(len(df)-1)\n",
        "#embeddings = np.load(update_path)[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwuHAyLkFF68"
      },
      "outputs": [],
      "source": [
        "#print(reworked_ticket_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGyVypXhFAzX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call the check_new_ticket function\n",
        "df, new_ticket_index = check_new_ticket(embeddings, reworked_ticket_text, df, update_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgTIeqxeKoGY"
      },
      "outputs": [],
      "source": [
        "plot_umap(embeddings, df['cluster_label'], new_ticket_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xaY97q2NOWj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9YSKhdbCAUw"
      },
      "outputs": [],
      "source": [
        " # Perform clustering\n",
        "\n",
        "#clusters, Z = perform_hierarchical_clustering(embeddings, MAX_DISTANCE)\n",
        "\n",
        "#df['cluster_label'] = clusters  # Assign cluster labels to DataFrame\n",
        "\n",
        "# Visualize using UMAP\n",
        "#visualize_umap(embeddings, clusters)\n",
        "\n",
        "# Generate word clouds for each cluster\n",
        "generate_wordclouds(df)\n",
        "\n",
        "\n",
        "# Generate labels using TF-IDF\n",
        "labels = generate_cluster_labels(df)\n",
        "\n",
        "\n",
        "# Create meaningful labels using GPT-3.5 Turbo\n",
        "cluster_labels_reworked = create_gpt3_labels(labels)\n",
        "\n",
        "\n",
        "# Create and display final cluster information\n",
        "cluster_data = pd.DataFrame({\n",
        "    'ClusterNumber': range(1, len(cluster_labels_reworked) + 1),\n",
        "    'ClusterLabel': cluster_labels_reworked,\n",
        "    'OriginalLabels': labels\n",
        "})\n",
        "\n",
        "print(cluster_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SrJoJMJebuP"
      },
      "outputs": [],
      "source": [
        "# Display all rows where 'cluster_label' equals 5\n",
        "filtered_df = df[df['cluster_label'] == 6]\n",
        "\n",
        "print(filtered_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umLmBqL3ekDW"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''\n",
        "def plot_umap_updated(embeddings, cluster_labels):\n",
        "    # Initialize UMAP\n",
        "    print(\"LABELS: \", cluster_labels)\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    # Transform the embeddings to 2D for visualization\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', s=50, alpha=0.6)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('UMAP Projection of Clusters')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'embeddings' and 'cluster_labels' are defined\n",
        "plot_umap_updated(embeddings, df['cluster_label'])\n",
        "\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Example DataFrame 'cluster_data' which you should already have\n",
        "# cluster_data = pd.DataFrame({\n",
        "#     'ClusterNumber': [1, 2, 3, 4, 5],\n",
        "#     'ClusterLabel': ['Tech Support', 'Product Feedback', 'Complaints', 'Inquiries', 'Other'],\n",
        "#     'OriginalLabels': ['support', 'feedback', 'complaint', 'inquiry', 'other']\n",
        "# })\n",
        "\n",
        "# Assuming cluster_data is predefined as above\n",
        "label_map = cluster_data.set_index('ClusterNumber')['ClusterLabel'].to_dict()\n",
        "\n",
        "# Define a list of colors (or use a matplotlib colormap)\n",
        "colors = plt.cm.Spectral(np.linspace(0, 1, len(cluster_data)))\n",
        "color_map = dict(zip(cluster_data['ClusterNumber'], colors))\n",
        "\n",
        "def plot_umap_updated(embeddings, cluster_ids):\n",
        "    # Initialize UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Iterate over each unique cluster id found in the dataset\n",
        "    for cluster_id in np.unique(cluster_ids):\n",
        "        # Find points that belong to the current cluster\n",
        "        idx = np.where(cluster_ids == cluster_id)\n",
        "        # Use label and color mappings, default to 'grey' if not found\n",
        "        cluster_label = label_map.get(cluster_id, 'Unknown')\n",
        "        cluster_color = color_map.get(cluster_id, 'grey')\n",
        "\n",
        "        # Plot each cluster using assigned colors and add label for the legend\n",
        "        plt.scatter(embedding_2d[idx, 0], embedding_2d[idx, 1], color=cluster_color, label=cluster_label, s=50, alpha=0.6)\n",
        "\n",
        "    plt.legend(title='Cluster Labels')\n",
        "    plt.title('UMAP Projection of Clusters with Custom Labels')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'embeddings' and 'cluster_ids' are defined appropriately\n",
        "plot_umap_updated(embeddings, df['cluster_label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PywjdVuuxeia"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import umap.umap_ as umap\n",
        "\n",
        "def plot_umap_interactive(embeddings, cluster_ids, titles):\n",
        "    # Initialize UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create a DataFrame for the Plotly scatter plot\n",
        "    df_plot = pd.DataFrame({\n",
        "        'UMAP Dimension 1': embedding_2d[:, 0],\n",
        "        'UMAP Dimension 2': embedding_2d[:, 1],\n",
        "        'Cluster': [label_map[cid] for cid in cluster_ids],\n",
        "        'Title': titles\n",
        "    })\n",
        "\n",
        "    # Create a scatter plot\n",
        "    fig = px.scatter(df_plot, x='UMAP Dimension 1', y='UMAP Dimension 2',\n",
        "                     color='Cluster', labels={'color': 'Cluster Label'},\n",
        "                     hover_data=['Title'],\n",
        "                     title='UMAP Projection of Clusters')\n",
        "    fig.update_traces(marker=dict(size=12, line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
        "    fig.show()\n",
        "\n",
        "# Example usage\n",
        "plot_umap_interactive(embeddings, df['cluster_label'], df['issue_title'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dwMsxhOxiWq"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EewTY5c_dPWw"
      },
      "source": [
        "**3. Adaptability and accuracy of the system**\n",
        "Goal: Without manual intervention, the system should not only correctly categorize new tickets with\n",
        "new content, but also adapt the ticket categories accordingly.\n",
        "Deliverable: A real-time classification engine that adapts to new data in real time. This module is\n",
        "further developed through transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "1LRMVaKNS82n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "84f791d1-ea8c-43d9-8c93-9318a94fb3ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'cluster_label'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'cluster_label'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4b4e81ab8b60>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplot_tsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cluster_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3761\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3763\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'cluster_label'"
          ]
        }
      ],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def plot_tsne(embeddings, cluster_labels):\n",
        "    # Initialize t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    # Transform the embeddings to 2D\n",
        "    embedding_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', s=50, alpha=0.6)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('t-SNE Projection of Clusters')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "plot_tsne(embeddings, df['cluster_label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLAYGROUND"
      ],
      "metadata": {
        "id": "_6habOqNQNYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_kmeans_clustering(data, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
        "    clusters = kmeans.fit_predict(data)\n",
        "    return clusters\n",
        "\n",
        "def perform_hierarchical_clustering(data, max_distance=2):\n",
        "    Z = linkage(data, method='ward')\n",
        "    clusters = fcluster(Z, max_distance, criterion='distance')\n",
        "    return clusters\n",
        "\n",
        "def perform_spectral_clustering(data, n_clusters=3):\n",
        "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42)\n",
        "    clusters = spectral.fit_predict(data)\n",
        "    return clusters\n"
      ],
      "metadata": {
        "id": "CLAC7FkvQPdF"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WJX9_BHRTPzM"
      },
      "outputs": [],
      "source": [
        "# Define configurations\n",
        "data_config = {\n",
        "    'small': DF_SMALL,\n",
        "    'mid': DF_MID,\n",
        "    'large': DF_LARGE,\n",
        "}\n",
        "\n",
        "embedding_config = {\n",
        "    'small_initial': EMBEDDINGS_SMALL,\n",
        "    'mid_initial': EMBEDDINGS_MID,\n",
        "    # Add more configurations here\n",
        "}\n",
        "\n",
        "clustering_config = {\n",
        "  'dbscan': perform_dbscan_clustering,\n",
        "    'kmeans': perform_kmeans_clustering,\n",
        "    'ward': perform_hierarchical_clustering,\n",
        "    'spectral': perform_spectral_clustering,\n",
        "    # Add more clustering methods here\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your functions here\n",
        "def load_data(config_key):\n",
        "    return load_datas(config_key)"
      ],
      "metadata": {
        "id": "_X5CAV67_YiU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOf-5YfO_4Vv"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df = drop_columns(df, ['journal_id', 'journal_created_on', 'journal_author', 'journal_notes', 'issue_id'])\n",
        "    df = replace_special_signs(df, 'issue_description')\n",
        "    df = combine_columns(df, ['issue_title', 'issue_description'], 'combined_text')\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "hdZ599TB_dYN"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rework_datas(df):\n",
        "  detailed_instructions = (\"You are a highly capable technical supporter. For each issue, provide a summary \"\n",
        "                          \"categorized under the following headers:\\n\"\n",
        "                          \"- Issue: [Specify the type of issue e.g., Support, IT-Systems, Development]\\n\"\n",
        "                          \"- Affected Systems: [List affected systems e.g., VDI, Email Servers, etc.]\\n\"\n",
        "                          \"- Reported Problem: [Describe the reported problem e.g., no connection, slow performance, etc.]\")\n",
        "\n",
        "  df = rework_ticket_information(file_path, df, detailed_instructions)\n",
        "  return df"
      ],
      "metadata": {
        "id": "g0AzruyE_de0"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_datas():\n",
        "  df = clean_datas(df, 'reworked_information')\n",
        "  df.head(1)"
      ],
      "metadata": {
        "id": "laQY0ly5_dtN"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def (EMBEDDING):\n",
        "  texts = df['reworked_information'].tolist()\n",
        "  embeddings = get_embeddings_impl(texts, EMBEDDING)"
      ],
      "metadata": {
        "id": "lqQc5BfK_dzt"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_data(embeddings):\n",
        "   perform_dbscan_clustering(embeddings)\n",
        "   perform_hierarchical_clustering(embeddings)\n",
        "   perform_kmeans_clustering(embeddings)\n",
        "   perform_spectral_clustering(embeddings)\n"
      ],
      "metadata": {
        "id": "0nbP3euG_d4b"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_clusters(embeddings, clusters):\n",
        "    # Your evaluation code here\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "QU8pPBLd_viH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clusters(embeddings, clusters):\n",
        "    # Your visualization code here\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "id": "nzyKe3KU_vn3"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TESTING"
      ],
      "metadata": {
        "id": "TeiqbO79gva0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING\n",
        "# Create widgets for interactivity\n",
        "\n",
        "df = load_data(DF_SMALL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JniCOD9yA724",
        "outputId": "bc7e639a-73f5-45b1-8c92-bd0cd3d28157"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./drive/Othercomputers/X1/code/datas_small.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_data(df)\n"
      ],
      "metadata": {
        "id": "5hHpxcwXBgTV"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = rework_datas(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoUi_OGVf82j",
        "outputId": "9ab484e7-fec0-4082-a4bb-7e1c006f03d8"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists. Loaded existing data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = get_embeddings(EMBEDDINGS_SMALL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4p6rVduggLp",
        "outputId": "c8a8ab7d-5d3f-4330-aee1-51a529890e0d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EMBEDDING read:  315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "d8ViRoMajTOU",
        "outputId": "2da39d00-8281-4ea5-d292-89819b749659"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IM PERFORM DBSCAN\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected 2D array, got scalar array instead:\narray=None.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-9cda098dadba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-119-22b3fc55c1b8>\u001b[0m in \u001b[0;36mcluster_data\u001b[0;34m(embeddings)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcluster_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mperform_dbscan_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mperform_hierarchical_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mperform_kmeans_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0mperform_spectral_clustering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-33854f8096fd>\u001b[0m in \u001b[0;36mperform_dbscan_clustering\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"IM PERFORM DBSCAN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mdbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_dbscan.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mCluster\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mNoisy\u001b[0m \u001b[0msamples\u001b[0m \u001b[0mare\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;31m# If input is scalar raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    895\u001b[0m                     \u001b[0;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=None.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an interactive method to set configurations\n",
        "def run_analysis(data_key, embedding_key, cluster_key):\n",
        "    df = load_data(data_config[data_key])\n",
        "    df_preprocessed = preprocess_data(df)\n",
        "    embeddings = get_embeddings(small_initial, df_preprocessed['cleaned_information'])\n",
        "    clusters = cluster_data(clustering_config[cluster_key], embeddings)\n",
        "    scores = evaluate_clusters(embeddings, clusters)\n",
        "    visualize_clusters(embeddings, clusters)\n",
        "    # Here you would log/display the scores for comparison\n",
        "\n",
        "# Create widgets for interactivity\n",
        "widgets.interact(run_analysis,\n",
        "                 data_key=widgets.Dropdown(options=data_config.keys()),\n",
        "                 embedding_key=widgets.Dropdown(options=embedding_config.keys()),\n",
        "                 cluster_key=widgets.Dropdown(options=clustering_config.keys()))\n",
        "\n"
      ],
      "metadata": {
        "id": "C10eoSU2_vtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKCDX_TB_vzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxyyNIiV_v5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULwM1GCNPphf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "e7DxzokNC9D5",
        "LBwl74V5DFtC",
        "najjKVvEyQsq",
        "peYs16m3EhmV",
        "hijF2duoztdt"
      ],
      "mount_file_id": "1twckJoyo6w0GKHcQqeSlp9zCqZN10pPC",
      "authorship_tag": "ABX9TyNeFKk2MbDQGytT2yOVkv3I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
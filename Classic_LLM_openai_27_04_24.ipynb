{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andipasti/smupo/blob/main/Classic_LLM_openai_27_04_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNSHHKfaCvHu"
      },
      "source": [
        "# MASTER THESIS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DxzokNC9D5"
      },
      "source": [
        "### INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iAHuRdyoMU_e"
      },
      "outputs": [],
      "source": [
        "#SETTING GLOBALS\n",
        "\n",
        "DF_SMALL = './drive/Othercomputers/X1/code/datas_small.json'\n",
        "DF_MID = './drive/Othercomputers/X1/code/datas_mid.json'\n",
        "DF_LARGE = './drive/Othercomputers/X1/code/datas_large.json'\n",
        "\n",
        "DF_SMALL_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_small.json'\n",
        "DF_MID_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_mid.json'\n",
        "DF_LARGE_SUM = './drive/Othercomputers/X1/code/processed_summarization_per_ticket_large.json'\n",
        "\n",
        "EMBEDDINGS_SMALL = './drive/Othercomputers/X1/code/embeddings/embeddings-small-initial.npy'\n",
        "EMBEDDINGS_MID = './drive/Othercomputers/X1/code/embeddings/embeddings-mid-initial.npy'\n",
        "EMBEDDINGS_LARGE = './drive/Othercomputers/X1/code/embeddings/embeddings-large-initial.npy'\n",
        "\n",
        "EMBEDDINGS_SMALL_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "EMBEDDINGS_MID_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "EMBEDDINGS_LARGE_UPDATED = './drive/Othercomputers/X1/code/embeddings/embeddings-updated-mid.npy'\n",
        "\n",
        "MAX_DISTANCE = 2;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import openai\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import re\n",
        "\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "import umap\n",
        "\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2rVdhzOmTYlB"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jgv2mj-CVxL",
        "outputId": "8542545a-54b9-47c2-a706-838cab0d1142",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.4.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.10/dist-packages (1.9.3)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from wordcloud) (1.25.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from wordcloud) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from wordcloud) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# INSTALL NEEDED LIBRARIES\n",
        "!pip install openai==0.28\n",
        "!pip install umap-learn\n",
        "!pip install wordcloud\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizers\n",
        "nltk.download('stopwords')  # Stopwords list for English and German\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Set up stopwords for both German and English\n",
        "stop_words = set(stopwords.words('english')).union(set(stopwords.words('german')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0dW94RB9B4Iu"
      },
      "outputs": [],
      "source": [
        "# CREATE ACCESS TO SECRETS\n",
        "# Now you can proceed with your code that requires this environment variable\n",
        "from google.colab import userdata\n",
        "os.environ['HUGGINGFACE_TOKEN'] = userdata.get('HUGGINGFACE_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2usZnsPDTPX",
        "outputId": "a71813f4-c008-419f-97b7-71a70e827233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n",
        "print(\"DONE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7fGWY63DT_4",
        "outputId": "5def8a7b-4622-42aa-9975-539393b8931a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key is set.\n"
          ]
        }
      ],
      "source": [
        "# CHECKING\n",
        "\n",
        "# Check if the OPENAI_API_KEY environment variable is set\n",
        "if \"OPENAI_API_KEY\" in os.environ:\n",
        "    print(\"OpenAI API key is set.\")\n",
        "else:\n",
        "    print(\"OpenAI API key is not set.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8x5_GAVb7BCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d2bdc9-8cd4-4689-e48b-f10ba20f2ed5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# CONNECT TO DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmY0vi2TDaUJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBwl74V5DFtC"
      },
      "source": [
        "### GETTING DATAS AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jSfkp7fiDYFi"
      },
      "outputs": [],
      "source": [
        "# LOAD DATAS\n",
        "def load_datas(config_key):\n",
        "  print(config_key)\n",
        "  df = pd.read_json(config_key)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "u0bVk2l_Ddun"
      },
      "outputs": [],
      "source": [
        "def print_column_names(df):\n",
        "  column_names = list(df.columns)\n",
        "  print(column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sPTyUg0_Dfii"
      },
      "outputs": [],
      "source": [
        "def drop_columns(df, columns):\n",
        "  return df.drop(columns=columns, errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GNCqVvbEpbZj"
      },
      "outputs": [],
      "source": [
        "def replace_special_signs(df, column):\n",
        "  df[column] = df[column].str.replace(r\"[\\n\\r]\", \" \", regex=True)\n",
        "  df[column] = df[column].str.replace(r\"[^a-zA-Z0-9\\s.,;!?äüöéèà']\", \"\", regex=True)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MyYaQBSBmeWK"
      },
      "outputs": [],
      "source": [
        "def combine_columns(df, source_cols, new_col_name, separator=\" \"):\n",
        "  df[new_col_name] = df[source_cols].astype(str).agg(separator.join, axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "najjKVvEyQsq"
      },
      "source": [
        "### RESTRUCTURE TICKET INFORMATION WITH HELP OF LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kv3AfNCjy7Ux"
      },
      "outputs": [],
      "source": [
        "def rework_ticket_description_with_gptturbo(ticket_text, detailed_instructions, model):\n",
        "    \"\"\"\n",
        "    Reworks a ticket description into a structured summary using the specified GPT model.\n",
        "\n",
        "    :param ticket_text: The text of the ticket.\n",
        "    :param detailed_instructions: Detailed instructions for the model.\n",
        "    :param model: The model identifier to use.\n",
        "    :return: Reworked ticket text.\n",
        "    \"\"\"\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": detailed_instructions},\n",
        "                {\"role\": \"user\", \"content\": ticket_text}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if response and 'choices' in response and len(response['choices']) > 0:\n",
        "            reworked_text = response['choices'][0].get('message', {}).get('content', '')\n",
        "            return reworked_text.strip()\n",
        "        else:\n",
        "            return \"Failed to generate reworked text. No valid response.\"\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CC9TX39JDs6w"
      },
      "outputs": [],
      "source": [
        "def rework_ticket_information(file_path, df, detailed_instructions, model=\"gpt-3.5-turbo\"):\n",
        "    \"\"\"\n",
        "    Check if a processed file exists. If not, process the tickets using GPT-Turbo and save the new data.\n",
        "\n",
        "    :param file_path: Path to the processed file.\n",
        "    :param df: DataFrame containing the ticket descriptions.\n",
        "    :param detailed_instructions: Instructions for GPT model summarization.\n",
        "    :param model: The model identifier to use with OpenAI's API.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        # If the file does not exist, apply the function to rework ticket descriptions\n",
        "        df['reworked_information'] = df['combined_text'].apply(lambda x: rework_ticket_description_with_gptturbo(x, detailed_instructions, model))\n",
        "        df.to_json(file_path, orient='records', lines=True)\n",
        "        files.download(file_path)\n",
        "        print(\"Processed new data and saved to file.\")\n",
        "    else:\n",
        "        # Load the existing file if it already exists\n",
        "        df = pd.read_json(file_path, lines=True)\n",
        "        print(\"File exists. Loaded existing data.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kHzdJsnIFyYM"
      },
      "outputs": [],
      "source": [
        "def clean_datas_in_col(text):\n",
        "    \"\"\" Cleans the text data by removing specific words and non-alphabetical characters. \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if text is not a string\n",
        "\n",
        "    text = text.lower()\n",
        "    specific_unwanted_words = ['reported problem', 'ticket', 'summary', 'affected systems', 'reported problem', 'issue']\n",
        "    pattern = r'\\b(?:' + '|'.join(specific_unwanted_words) + r')\\b'\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def clean_datas(df, column_name):\n",
        "    \"\"\" Applies text cleaning to a specified column in the DataFrame. \"\"\"\n",
        "    df[column_name + '_cleaned'] = df[column_name].apply(clean_datas_in_col)\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peYs16m3EhmV"
      },
      "source": [
        "## CREATE EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-qivnbJ8Vdaj"
      },
      "outputs": [],
      "source": [
        "# USE OPENAI MODEL TO CREATE EMBEDDINGS OUT OF REWORKED INFORMATION\n",
        "\n",
        "# Function to get embeddings from OpenAI\n",
        "def _impl(texts, embeddings_file_path):\n",
        "\n",
        "  # Explicitly set the OpenAI API key\n",
        "  openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "  if not os.path.exists(embeddings_file_path):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        response = openai.Embedding.create(\n",
        "            input=text,\n",
        "            engine=\"text-embedding-ada-002\"\n",
        "        )\n",
        "        embeddings.append(response['data'][0]['embedding'])\n",
        "\n",
        "    # save:\n",
        "    np.save(embeddings_file_path, embeddings)\n",
        "  else:\n",
        "    embeddings = np.load(embeddings_file_path)\n",
        "    print(\"EMBEDDING read: \", embeddings.shape[0])\n",
        "\n",
        "  return np.array(embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30CUT37DImGB"
      },
      "source": [
        "TODO:  \n",
        "- different models\n",
        "- optimize preprocessing (normalize, stopwords, lemmatization,...)\n",
        "- fine-tuning embeddings /hybrid approach (using z.b randomForest after clustering)\n",
        "- dimensionality reduction (PCA f.e.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ckKNThE5tT"
      },
      "source": [
        "## CREATE CLUSTERS V1\n",
        "\n",
        "### WARD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "h8ACXwP7m5yI"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dendrogram(Z, title='Hierarchical Clustering Dendrogram', xlabel='Sample index or (Cluster size)', ylabel='Distance'):\n",
        "    \"\"\"\n",
        "    Plot a dendrogram to visualize hierarchical clustering results.\n",
        "\n",
        "    :param Z: Linkage matrix from hierarchical clustering.\n",
        "    :param title: Title of the dendrogram.\n",
        "    :param xlabel: Label for the x-axis.\n",
        "    :param ylabel: Label for the y-axis.\n",
        "    \"\"\"\n",
        "    if Z is None:\n",
        "        print(\"No linkage matrix provided for plotting.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(25, 10))\n",
        "    dendrogram(\n",
        "        Z,\n",
        "        leaf_rotation=90.,  # rotates the x axis labels\n",
        "        leaf_font_size=8.,  # font size for the x axis labels\n",
        "    )\n",
        "    plt.title(title)\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(ylabel)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IEUdu04hS7_K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DBSCAN"
      ],
      "metadata": {
        "id": "2eJVGbEUQZhJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ijRAk70d-Azf"
      },
      "outputs": [],
      "source": [
        "# Optional: Plot clusters if you wish to visualize\n",
        "# plot_clusters_with_umap(matrix, clusters)\n",
        "\n",
        "\n",
        "def plot_clusters(matrix, clusters):\n",
        "    # Perform dimensionality reduction for visualization if it's not 2D\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    matrix_2d = tsne.fit_transform(matrix)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(matrix_2d[:, 0], matrix_2d[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('DBSCAN Clustering Visualization')\n",
        "    plt.xlabel('TSNE Dimension 1')\n",
        "    plt.ylabel('TSNE Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "def plot_clusters_with_umap(matrix, clusters):\n",
        "    # Use UMAP for dimensionality reduction\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    matrix_2d = reducer.fit_transform(matrix)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    scatter = plt.scatter(matrix_2d[:, 0], matrix_2d[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('DBSCAN Clustering Visualization with UMAP')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nbE_G5lzbKme"
      },
      "outputs": [],
      "source": [
        "# Assuming 'matrix' is your embeddings matrix obtained from get_openai_embeddings function\n",
        "def perform_kmeans_clustering(matrix, num_clusters):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, init='k-means++', random_state=42)\n",
        "    clusters = kmeans.fit_predict(matrix)\n",
        "    return clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_dbscan_clustering(matrix, eps=0.5, min_samples=5):\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    clusters = dbscan.fit_predict(matrix)\n",
        "    return clusters\n"
      ],
      "metadata": {
        "id": "nEMNY9IfnmDQ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_spectral_clustering(matrix, n_clusters):\n",
        "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors',\n",
        "                                  assign_labels='kmeans', random_state=42)\n",
        "    clusters = spectral.fit_predict(matrix)\n",
        "    return clusters\n"
      ],
      "metadata": {
        "id": "CTslVfmWTvMG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "FF70HCiixtAY"
      },
      "outputs": [],
      "source": [
        "def perform_hierarchical_clustering(matrix, num_clusters, method='ward'):\n",
        "    Z = linkage(matrix, method=method)\n",
        "    clusters = fcluster(Z, num_clusters, criterion='maxclust')\n",
        "    return clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrJIlT6BFXeC"
      },
      "source": [
        "### CHECK HOW MANY CLUSTERS WOULD BE USEFUL (KMEANS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cd3gKlbhm7II"
      },
      "outputs": [],
      "source": [
        "def calculate_wcss(matrix, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Calculate the Within-Cluster Sum of Squares (WCSS) for different numbers of clusters in K-Means.\n",
        "\n",
        "    :param matrix: 2D numpy array of data points.\n",
        "    :param max_clusters: Maximum number of clusters to test.\n",
        "    :return: List of WCSS values.\n",
        "    \"\"\"\n",
        "    wcss = []\n",
        "    for i in range(1, max_clusters + 1):\n",
        "        kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=15, random_state=0)\n",
        "        kmeans.fit(matrix)\n",
        "        wcss.append(kmeans.inertia_)\n",
        "    return wcss\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_elbow_curve(wcss, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Plot the elbow curve using WCSS values to help determine the optimal number of clusters.\n",
        "\n",
        "    :param wcss: List of WCSS values.\n",
        "    :param max_clusters: Maximum number of clusters tested, for x-axis range.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, max_clusters + 1), wcss, marker='o')\n",
        "    plt.title('Elbow Method for Determining Optimal Clusters')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "KA3Zhos5T3wo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qFciMxR4agAu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_umap(embeddings, clusters):\n",
        "    umap_reducer = umap.UMAP()\n",
        "    umap_result = umap_reducer.fit_transform(embeddings)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=clusters, cmap='viridis', alpha=0.7)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('UMAP Visualization of Clusters')\n",
        "    plt.xlabel('UMAP Feature 1')\n",
        "    plt.ylabel('UMAP Feature 2')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IfmYE1iKVF4y"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ooqwllfp5iH9"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Cleans text by removing specific unwanted words and filtering out non-alphabetical characters.\n",
        "\n",
        "    :param text: Text to be cleaned.\n",
        "    :return: Cleaned text as a string.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"  # Return empty string if text is not a string\n",
        "\n",
        "    # List of specific unwanted words\n",
        "    unwanted_words = ['reported problem', 'ticket', 'summary', 'affected systems', 'issue']\n",
        "    pattern = r'\\b(?:' + '|'.join(unwanted_words) + r')\\b'\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(pattern, '', text)\n",
        "\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def clean_datas(df, column_name):\n",
        "    \"\"\"\n",
        "    Applies text cleaning to a specified column in the DataFrame and updates the column directly.\n",
        "\n",
        "    :param df: DataFrame containing the data.\n",
        "    :param column_name: Name of the column to clean.\n",
        "    :return: DataFrame with cleaned text in the same column.\n",
        "    \"\"\"\n",
        "    df[column_name + '_cleaned'] = df[column_name].apply(clean_text)\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVj9le13Uot-"
      },
      "source": [
        "CHECKPOINT 1: EVALUATE CLUSTERING"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fLORVenjQu9P"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_clusters_with_umap(matrix, clusters, title):\n",
        "    umap_reducer = umap.UMAP(random_state=42)\n",
        "    umap_result = umap_reducer.fit_transform(matrix)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(umap_result[:, 0], umap_result[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('UMAP Feature 1')\n",
        "    plt.ylabel('UMAP Feature 2')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "oRsUnH70QyHi"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRHWnr4cQ0Dp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "YqQjUjPAUumK"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yff6ERg0b01k"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "yCmic5nZXI9d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "KFgdBQM-U0x0"
      },
      "outputs": [],
      "source": [
        "# Davies-Bouldin Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej180bnULQF3"
      },
      "source": [
        "SHOWING IN A WORD CLOUD - CLUSTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "WUmX6uWZLPaP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "PNmOjtIDu05P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "yS5N9k8xLXa6"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def generate_wordclouds(df):\n",
        "    unique_clusters = np.unique(df['cluster_label'])\n",
        "    print(\"UNIQUE CLUSTERS: \", unique_clusters)\n",
        "    for cluster_num in unique_clusters:\n",
        "\n",
        "        cluster_data = df[df['cluster_label'] == cluster_num]\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_data['cleaned_information'])\n",
        "        terms = tfidf_vectorizer.get_feature_names_out()\n",
        "        scores = tfidf_matrix.sum(axis=0).A1\n",
        "        freqs = dict(zip(terms, scores))\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(freqs)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f'Word Cloud for Cluster {cluster_num}')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "6CO44BupiShP"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def generate_cluster_labels(df):\n",
        "    unique_clusters = np.unique(df['cluster_label'])\n",
        "    labels = []\n",
        "    print(\"CLUSTERS IN GENERATE CLUSTER FUNC \", unique_clusters)\n",
        "    for cluster_num in unique_clusters:\n",
        "\n",
        "        cluster_data = df[df['cluster_label'] == cluster_num]\n",
        "\n",
        "        tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(cluster_data['cleaned_information'])\n",
        "        terms = np.array(tfidf_vectorizer.get_feature_names_out())[tfidf_matrix.sum(axis=0).argsort()[0, ::-1]].flat[:5]\n",
        "        label = ', '.join(terms)\n",
        "        labels.append(label)\n",
        "        print(f\"Cluster {cluster_num} label: {label}\")\n",
        "    return labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "XY840vaXHZ-m"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# Generate meaningful labels for each cluster using GPT-3.5 Turbo\n",
        "def create_gpt3_labels(labels):\n",
        "    cluster_labels = []\n",
        "    for label in labels:\n",
        "\n",
        "        #prompt = \"Please generate a concise label for the following cluster of terms.\"\n",
        "        prompt = f\"Generate a very specific and descriptive maximum 3-words-label for a cluster of topics involving: {label}. Aim for precision and relevance in a technical context. it is all about a webapplication where users like teachers or scholars can see timetable, do enrolments, add and confirm absences and many more.\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt},\n",
        "                {\"role\": \"user\", \"content\": label}\n",
        "            ]\n",
        "        )\n",
        "        new_label = response['choices'][0]['message']['content'].strip()\n",
        "        cluster_labels.append(new_label)\n",
        "    return cluster_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "D1mbKxmWHxDA"
      },
      "outputs": [],
      "source": [
        "def generate_wordclouds(df, text_column, label_column):\n",
        "    \"\"\"\n",
        "    Generates and displays word clouds for each cluster.\n",
        "\n",
        "    :param df: DataFrame containing the cluster labels and text data.\n",
        "    :param text_column: Name of the column in df that contains the text data.\n",
        "    :param label_column: Name of the column in df that contains the cluster labels.\n",
        "    \"\"\"\n",
        "    unique_clusters = df[label_column].unique()\n",
        "    for cluster in unique_clusters:\n",
        "        cluster_text = \" \".join(df[df[label_column] == cluster][text_column].dropna())\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Word Cloud for Cluster {cluster}\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "wJdAcuyStmda"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rfMxWQR8ehQ"
      },
      "source": [
        "## CATEGORIZING NEW TICKETS TO ONE OF THE CLUSTERS WITHOUT THE NEED OF DOING THE WHOLE CLUSTERSTUFF ETC AGAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "a6mC3W6kLOJU"
      },
      "outputs": [],
      "source": [
        "def plot_umap(embeddings, cluster_labels, new_ticket_index):\n",
        "    reducer = umap.UMAP()\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "    plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', alpha=0.6)\n",
        "    plt.scatter(embedding_2d[new_ticket_index, 0], embedding_2d[new_ticket_index, 1], c='red', s=100, edgecolors='k')  # Highlight new ticket\n",
        "    plt.title(\"UMAP Projection of Ticket Embeddings\")\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n4w5-xAkrxxc"
      },
      "outputs": [],
      "source": [
        "def check_new_ticket(embeddings, reworked_ticket_text, df, update_path, threshold=0.5):\n",
        "    print(\"Starting check_new_ticket...\")\n",
        "\n",
        "    # get last embedding\n",
        "    new_embedding = embeddings[-1]\n",
        "    all_embeddings_exept_last = embeddings[:-1]\n",
        "\n",
        "    # Check if the DataFrame is empty or the existing_embeddings are insufficient\n",
        "    if df.empty or len(embeddings) <= 1:  # Ensure there's at least one existing ticket besides the new one\n",
        "        print(\"No existing tickets to compare or DataFrame is empty.\")\n",
        "        new_label = 0  # Starting with a new cluster if no valid data exists\n",
        "    else:\n",
        "        # Exclude the new embedding which is presumably the last one added\n",
        "        print(new_embedding)\n",
        "        # Calculate distances to all existing embeddings\n",
        "        distances = euclidean_distances([new_embedding], all_embeddings_exept_last)\n",
        "        min_distance = distances.min()\n",
        "\n",
        "        print(\"Distance: \", distances.min())\n",
        "        # Decide on the cluster label based on distance\n",
        "        if min_distance > threshold:\n",
        "          print(\"NEUES CLUSTER\")\n",
        "          new_label = df['cluster_label'].max() + 1 if not df.empty else 0\n",
        "          #new_label = 0\n",
        "          print(f\"Assigning new cluster label: {new_label}\")\n",
        "        else:\n",
        "            print(\"BESTEHENDES CLUSTER\")\n",
        "            closest_index = distances.argmin()\n",
        "            print(\"CLOSEST INDEX: \", closest_index)\n",
        "            if closest_index < len(df):\n",
        "                new_label = df.iloc[closest_index]['cluster_label']\n",
        "                #new_label = 0\n",
        "                print(f\"Assigning existing cluster label: {new_label}\")\n",
        "            else:\n",
        "                print(f\"Error: Closest index {closest_index} out of bounds for DataFrame of length {len(df)}\")\n",
        "                return df, None  # Return None if there's an index error\n",
        "\n",
        "    # Append the new ticket's data as a new row to the DataFrame\n",
        "\n",
        "    df.loc[len(df)-1, 'cluster_label'] =  new_label\n",
        "\n",
        "    new_ticket_index = df.index[-1]  # Get the index of the newly added ticket\n",
        "    return df, new_ticket_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ER20SqhMr0aB"
      },
      "outputs": [],
      "source": [
        "def get_openai_embeddings_addon(texts, initial_path, update_path):\n",
        "    # Check and load initial embeddings or initialize an empty array\n",
        "\n",
        "    new_embeddings = []\n",
        "    for text in texts:\n",
        "        response = openai.Embedding.create(input=text, engine=\"text-embedding-3-large\")\n",
        "        new_embeddings.append(response['data'][0]['embedding'])\n",
        "\n",
        "    return new_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hijF2duoztdt"
      },
      "source": [
        "## CREATE ANOTHER TICKET AND RECLUSTER IT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gw40kkhx81WU",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "2d8f2e1d-e10f-4789-e702-dc247650d737"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'EMBEDDINGS_MID' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c6aedc4337a7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minitial_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMBEDDINGS_MID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mupdate_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMBEDDINGS_MID_UPDATED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew_ticket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"eine gurke kommt plötzlich aus meinem handy anstelle einer banane.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreworked_ticket_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrework_ticket_description_with_gptturbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_ticket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EMBEDDINGS_MID' is not defined"
          ]
        }
      ],
      "source": [
        "initial_path = EMBEDDINGS_MID\n",
        "update_path = EMBEDDINGS_MID_UPDATED\n",
        "new_ticket = \"eine gurke kommt plötzlich aus meinem handy anstelle einer banane.\"\n",
        "\n",
        "reworked_ticket_text = rework_ticket_description_with_gptturbo(new_ticket)\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")\n",
        "df.loc[len(df), ['issue_title', 'reworked_information', 'cleaned_information']] = [\"MY TEST TITLE FOR NEW ISSUES\", reworked_ticket_text, reworked_ticket_text]\n",
        "\n",
        "\n",
        "new_embeddings = get_openai_embeddings_addon([reworked_ticket_text], initial_path, update_path)[-1]\n",
        "\n",
        "# Combine new embeddings with existing ones\n",
        "new_embeddings = np.array(new_embeddings)\n",
        "embeddings = np.vstack([embeddings, new_embeddings]) if embeddings.size else new_embeddings\n",
        "\n",
        "# Save updated embeddings\n",
        "np.save(update_path, embeddings)\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")\n",
        "\n",
        "#embeddings = np.load(update_path)[:-2]\n",
        "#embeddings = np.load(update_path)[:-1]\n",
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzVXwUoLaVEu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XDFzwwIGVtX"
      },
      "outputs": [],
      "source": [
        "print(f\"Length of existing_embeddings: {len(embeddings)}\")\n",
        "print(f\"Number of rows in df: {len(df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxjfkEc7P7SC"
      },
      "outputs": [],
      "source": [
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3G07cGGZ_tE"
      },
      "outputs": [],
      "source": [
        "#df = df.drop(len(df)-1)\n",
        "#embeddings = np.load(update_path)[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwuHAyLkFF68"
      },
      "outputs": [],
      "source": [
        "#print(reworked_ticket_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGyVypXhFAzX"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Call the check_new_ticket function\n",
        "df, new_ticket_index = check_new_ticket(embeddings, reworked_ticket_text, df, update_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgTIeqxeKoGY"
      },
      "outputs": [],
      "source": [
        "plot_umap(embeddings, df['cluster_label'], new_ticket_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xaY97q2NOWj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9YSKhdbCAUw"
      },
      "outputs": [],
      "source": [
        " # Perform clustering\n",
        "\n",
        "#clusters, Z = perform_hierarchical_clustering(embeddings, MAX_DISTANCE)\n",
        "\n",
        "#df['cluster_label'] = clusters  # Assign cluster labels to DataFrame\n",
        "\n",
        "# Visualize using UMAP\n",
        "#visualize_umap(embeddings, clusters)\n",
        "\n",
        "# Generate word clouds for each cluster\n",
        "generate_wordclouds(df)\n",
        "\n",
        "\n",
        "# Generate labels using TF-IDF\n",
        "labels = generate_cluster_labels(df)\n",
        "\n",
        "\n",
        "# Create meaningful labels using GPT-3.5 Turbo\n",
        "cluster_labels_reworked = create_gpt3_labels(labels)\n",
        "\n",
        "\n",
        "# Create and display final cluster information\n",
        "cluster_data = pd.DataFrame({\n",
        "    'ClusterNumber': range(1, len(cluster_labels_reworked) + 1),\n",
        "    'ClusterLabel': cluster_labels_reworked,\n",
        "    'OriginalLabels': labels\n",
        "})\n",
        "\n",
        "print(cluster_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SrJoJMJebuP"
      },
      "outputs": [],
      "source": [
        "# Display all rows where 'cluster_label' equals 5\n",
        "filtered_df = df[df['cluster_label'] == 6]\n",
        "\n",
        "print(filtered_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umLmBqL3ekDW"
      },
      "outputs": [],
      "source": [
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "'''\n",
        "def plot_umap_updated(embeddings, cluster_labels):\n",
        "    # Initialize UMAP\n",
        "    print(\"LABELS: \", cluster_labels)\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    # Transform the embeddings to 2D for visualization\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', s=50, alpha=0.6)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('UMAP Projection of Clusters')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'embeddings' and 'cluster_labels' are defined\n",
        "plot_umap_updated(embeddings, df['cluster_label'])\n",
        "\n",
        "'''\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap.umap_ as umap\n",
        "\n",
        "# Example DataFrame 'cluster_data' which you should already have\n",
        "# cluster_data = pd.DataFrame({\n",
        "#     'ClusterNumber': [1, 2, 3, 4, 5],\n",
        "#     'ClusterLabel': ['Tech Support', 'Product Feedback', 'Complaints', 'Inquiries', 'Other'],\n",
        "#     'OriginalLabels': ['support', 'feedback', 'complaint', 'inquiry', 'other']\n",
        "# })\n",
        "\n",
        "# Assuming cluster_data is predefined as above\n",
        "label_map = .set_index('ClusterNumber')['ClusterLabel'].to_dict()\n",
        "\n",
        "# Define a list of colors (or use a matplotlib colormap)\n",
        "colors = plt.cm.Spectral(np.linspace(0, 1, len(cluster_data)))\n",
        "color_map = dict(zip(cluster_data['ClusterNumber'], colors))\n",
        "\n",
        "def plot_umap_updated(embeddings, cluster_ids):\n",
        "    # Initialize UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # Iterate over each unique cluster id found in the dataset\n",
        "    for cluster_id in np.unique(cluster_ids):\n",
        "        # Find points that belong to the current cluster\n",
        "        idx = np.where(cluster_ids == cluster_id)\n",
        "        # Use label and color mappings, default to 'grey' if not found\n",
        "        cluster_label = label_map.get(cluster_id, 'Unknown')\n",
        "        cluster_color = color_map.get(cluster_id, 'grey')\n",
        "\n",
        "        # Plot each cluster using assigned colors and add label for the legend\n",
        "        plt.scatter(embedding_2d[idx, 0], embedding_2d[idx, 1], color=cluster_color, label=cluster_label, s=50, alpha=0.6)\n",
        "\n",
        "    plt.legend(title='Cluster Labels')\n",
        "    plt.title('UMAP Projection of Clusters with Custom Labels')\n",
        "    plt.xlabel('UMAP Dimension 1')\n",
        "    plt.ylabel('UMAP Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "# Assuming 'embeddings' and 'cluster_ids' are defined appropriately\n",
        "plot_umap_updated(embeddings, df['cluster_label'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PywjdVuuxeia"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import umap.umap_ as umap\n",
        "\n",
        "def plot_umap_interactive(embeddings, cluster_ids, titles):\n",
        "    # Initialize UMAP\n",
        "    reducer = umap.UMAP(random_state=42)\n",
        "    embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create a DataFrame for the Plotly scatter plot\n",
        "    df_plot = pd.DataFrame({\n",
        "        'UMAP Dimension 1': embedding_2d[:, 0],\n",
        "        'UMAP Dimension 2': embedding_2d[:, 1],\n",
        "        'Cluster': [label_map[cid] for cid in cluster_ids],\n",
        "        'Title': titles\n",
        "    })\n",
        "\n",
        "    # Create a scatter plot\n",
        "    fig = px.scatter(df_plot, x='UMAP Dimension 1', y='UMAP Dimension 2',\n",
        "                     color='Cluster', labels={'color': 'Cluster Label'},\n",
        "                     hover_data=['Title'],\n",
        "                     title='UMAP Projection of Clusters')\n",
        "    fig.update_traces(marker=dict(size=12, line=dict(width=2, color='DarkSlateGrey')), selector=dict(mode='markers'))\n",
        "    fig.show()\n",
        "\n",
        "# Example usage\n",
        "plot_umap_interactive(embeddings, df['cluster_label'], df['issue_title'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dwMsxhOxiWq"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EewTY5c_dPWw"
      },
      "source": [
        "**3. Adaptability and accuracy of the system**\n",
        "Goal: Without manual intervention, the system should not only correctly categorize new tickets with\n",
        "new content, but also adapt the ticket categories accordingly.\n",
        "Deliverable: A real-time classification engine that adapts to new data in real time. This module is\n",
        "further developed through transfer learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LRMVaKNS82n"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "def plot_tsne(embeddings, cluster_labels):\n",
        "    # Initialize t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    # Transform the embeddings to 2D\n",
        "    embedding_2d = tsne.fit_transform(embeddings)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    scatter = plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c=cluster_labels, cmap='Spectral', s=50, alpha=0.6)\n",
        "    plt.colorbar(scatter)\n",
        "    plt.title('t-SNE Projection of Clusters')\n",
        "    plt.xlabel('t-SNE Dimension 1')\n",
        "    plt.ylabel('t-SNE Dimension 2')\n",
        "    plt.show()\n",
        "\n",
        "plot_tsne(embeddings, df['cluster_label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PLAYGROUND"
      ],
      "metadata": {
        "id": "_6habOqNQNYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def perform_kmeans_clustering(data, n_clusters=3):\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
        "    clusters = kmeans.fit_predict(data)\n",
        "    return clusters\n",
        "\n",
        "def perform_hierarchical_clustering(data, max_distance=2):\n",
        "    Z = linkage(data, method='ward')\n",
        "    clusters = fcluster(Z, max_distance, criterion='distance')\n",
        "    return clusters\n",
        "\n",
        "def perform_spectral_clustering(data, n_clusters=3):\n",
        "    spectral = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', assign_labels='kmeans', random_state=42)\n",
        "    clusters = spectral.fit_predict(data)\n",
        "    return clusters\n",
        "'''"
      ],
      "metadata": {
        "id": "CLAC7FkvQPdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_clustering_metrics(matrix, labels):\n",
        "    # Check if there is more than one unique cluster and less than the length of the dataset\n",
        "    if len(set(labels)) > 1 and len(set(labels)) < len(matrix):\n",
        "        silhouette = silhouette_score(matrix, labels)\n",
        "        davies = davies_bouldin_score(matrix, labels)\n",
        "    else:\n",
        "        silhouette, davies = -1, -1  # Use -1 to indicate an invalid clustering evaluation\n",
        "    return silhouette, davies\n"
      ],
      "metadata": {
        "id": "cbEurNGifxkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_clustering_and_evaluation(matrix):\n",
        "    dbscan_clusters = perform_dbscan_clustering(matrix)\n",
        "    hierarchical_clusters = perform_hierarchical_clustering(matrix, num_clusters=5)\n",
        "    kmeans_clusters = perform_kmeans_clustering(matrix, num_clusters=5)\n",
        "    spectral_clusters = perform_spectral_clustering(matrix, num_clusters=5)\n",
        "\n",
        "    evaluate_clusters(matrix, dbscan_clusters, hierarchical_clusters, kmeans_clusters, spectral_clusters)\n",
        "\n"
      ],
      "metadata": {
        "id": "pdYSrHvLoZ1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Vbi3xuW1f1mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJX9_BHRTPzM"
      },
      "outputs": [],
      "source": [
        "# Define configurations\n",
        "data_config = {\n",
        "    'small': DF_SMALL,\n",
        "    'mid': DF_MID,\n",
        "    'large': DF_LARGE,\n",
        "}\n",
        "\n",
        "embedding_config = {\n",
        "    'small_initial': EMBEDDINGS_SMALL,\n",
        "    'mid_initial': EMBEDDINGS_MID,\n",
        "    # Add more configurations here\n",
        "}\n",
        "\n",
        "clustering_config = {\n",
        "  'dbscan': perform_dbscan_clustering,\n",
        "    'kmeans': perform_kmeans_clustering,\n",
        "    'ward': perform_hierarchical_clustering,\n",
        "    'spectral': perform_spectral_clustering,\n",
        "    # Add more clustering methods here\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your functions here\n",
        "def load_data(config_key):\n",
        "    return load_datas(config_key)"
      ],
      "metadata": {
        "id": "_X5CAV67_YiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FOf-5YfO_4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df = drop_columns(df, ['journal_id', 'journal_created_on', 'journal_author', 'journal_notes', 'issue_id'])\n",
        "    df = replace_special_signs(df, 'issue_description')\n",
        "    df = combine_columns(df, ['issue_title', 'issue_description'], 'combined_text')\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "hdZ599TB_dYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rework_datas(df):\n",
        "  detailed_instructions = (\"You are a highly capable technical supporter. For each issue, provide a summary \"\n",
        "                          \"categorized under the following headers:\\n\"\n",
        "                          \"- Issue: [Specify the type of issue e.g., Support, IT-Systems, Development]\\n\"\n",
        "                          \"- Affected Systems: [List affected systems e.g., VDI, Email Servers, etc.]\\n\"\n",
        "                          \"- Reported Problem: [Describe the reported problem e.g., no connection, slow performance, etc.]\")\n",
        "\n",
        "  df = rework_ticket_information(file_path, df, detailed_instructions)\n",
        "  return df"
      ],
      "metadata": {
        "id": "g0AzruyE_de0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(EMBEDDING):\n",
        "  texts = df['reworked_information'].tolist()\n",
        "  embeddings = get_embeddings_impl(texts, EMBEDDING)"
      ],
      "metadata": {
        "id": "lqQc5BfK_dzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cluster_data(matrix):\n",
        "    num_clusters = determine_optimal_clusters(matrix)\n",
        "\n",
        "    # Get cluster labels from each algorithm\n",
        "    dbscan_clusters = perform_dbscan_clustering(matrix)\n",
        "    hierarchical_clusters = perform_hierarchical_clustering(matrix, num_clusters)\n",
        "    kmeans_clusters = perform_kmeans_clustering(matrix, num_clusters)\n",
        "    spectral_clusters = perform_spectral_clustering(matrix, num_clusters)\n",
        "\n",
        "    return dbscan_clusters, hierarchical_clusters, kmeans_clusters, spectral_clusters\n"
      ],
      "metadata": {
        "id": "0nbP3euG_d4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_clusters(embeddings)\n",
        "  plot_clusters_with_umap(matrix, kmeans_clusters, 'K-Means Clustering with UMAP')\n",
        "  plot_clusters_with_umap(matrix, clusters, 'DBSCAN Clustering with UMAP')  # Assuming 'clusters' from DBSCAN\n",
        "  plot_clusters_with_umap(matrix, cluster_labels, 'Hierarchical Clustering with UMAP')  # Assuming 'cluster_labels' from Ward\n",
        "  plot_clusters_with_umap(matrix, spectral_clusters, 'Spectral Clustering with UMAP')"
      ],
      "metadata": {
        "id": "SSz-RZ5jbT6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_clusters(matrix, **cluster_labels):\n",
        "    for name, labels in cluster_labels.items():\n",
        "        silhouette, davies = compute_clustering_metrics(matrix, labels)\n",
        "        print(f\"{name} Silhouette: {silhouette}, {name} Davies-Bouldin: {davies}\")\n",
        ""
      ],
      "metadata": {
        "id": "QU8pPBLd_viH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XBHAFCeIyIMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_wordclouds_and_labels():\n",
        "  # Generate word clouds\n",
        "  generate_wordclouds(df, text_column=\"Text\", label_column=\"ClusterLabel\")\n",
        "\n",
        "  # Generate initial labels using TF-IDF or similar\n",
        "  initial_labels = generate_cluster_labels(df)\n",
        "\n",
        "  # Create meaningful labels using GPT-3.5 Turbo\n",
        "  cluster_labels_reworked = create_gpt3_labels(initial_labels)\n",
        "\n",
        "  # Create and display final cluster information\n",
        "  cluster_data = pd.DataFrame({\n",
        "      'ClusterNumber': range(1, len(cluster_labels_reworked) + 1),\n",
        "      'ClusterLabel': cluster_labels_reworked,\n",
        "      'OriginalLabels': initial_labels\n",
        "  })\n",
        "\n",
        "  print(cluster_data)\n"
      ],
      "metadata": {
        "id": "nt25RAYkyIO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "nzyKe3KU_vn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TESTING"
      ],
      "metadata": {
        "id": "TeiqbO79gva0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TESTING\n",
        "# Create widgets for interactivity\n",
        "\n",
        "df = load_data(DF_SMALL)\n"
      ],
      "metadata": {
        "id": "JniCOD9yA724"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_data(df)\n"
      ],
      "metadata": {
        "id": "5hHpxcwXBgTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = rework_datas(df)"
      ],
      "metadata": {
        "id": "AoUi_OGVf82j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_datas(df, 'reworked_information')"
      ],
      "metadata": {
        "id": "puvfgFO9airn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = get_embeddings(EMBEDDINGS_SMALL)"
      ],
      "metadata": {
        "id": "_4p6rVduggLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_data(embeddings)"
      ],
      "metadata": {
        "id": "d8ViRoMajTOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_clusters(matrix, DBSCAN=dbscan_clusters, KMeans=kmeans_clusters)\n"
      ],
      "metadata": {
        "id": "mt8JVYyNQiQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6ihHqa3ah2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an interactive method to set configurations\n",
        "def run_analysis(data_key, embedding_key, cluster_key):\n",
        "    df = load_data(data_config[data_key])\n",
        "    df_preprocessed = preprocess_data(df)\n",
        "    embeddings = get_embeddings(small_initial, df_preprocessed['cleaned_information'])\n",
        "    clusters = cluster_data(clustering_config[cluster_key], embeddings)\n",
        "    scores = evaluate_clusters(embeddings, clusters)\n",
        "    visualize_clusters(embeddings, clusters)\n",
        "    # Here you would log/display the scores for comparison\n",
        "\n",
        "# Create widgets for interactivity\n",
        "widgets.interact(run_analysis,\n",
        "                 data_key=widgets.Dropdown(options=data_config.keys()),\n",
        "                 embedding_key=widgets.Dropdown(options=embedding_config.keys()),\n",
        "                 cluster_key=widgets.Dropdown(options=clustering_config.keys()))\n",
        "\n"
      ],
      "metadata": {
        "id": "C10eoSU2_vtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eKCDX_TB_vzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MxyyNIiV_v5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ULwM1GCNPphf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "hijF2duoztdt"
      ],
      "mount_file_id": "1twckJoyo6w0GKHcQqeSlp9zCqZN10pPC",
      "authorship_tag": "ABX9TyOhOgDPtX0L42dWGw4mrPMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}